# (임시)Crawling
현재 진행 중인 크롤링 파트 task입니다.

## 현재 작업 중
- 크롤링된 데이터 손실 체크 기능 추가
- 로컬-aws간 통신 구축
- 대용량 파일 갱신 방법 찾기

- 레딧 크롤링
  - 대용량 아카이빙 데이터 탐색 최적화
  - 레딧 검색엔진으로 찾아낼 수 있는 게임 별 서브레딧 할당 - 리스트 추가로 수동 조정
  - 서브레딧도 없고, 스팀 리뷰도 적은 게임(1000위 근처)의 데이터를 종합게임 게시판에서라도 찾아낼 방법 탐색 중
  - 아카이빙 된 데이터에서 찾을 수 있는 컬럼은 https://lovit.github.io/dataset/2019/01/16/get_reddit/ 참조

## 해결된 문제
- 게임 리스트 작성 시, 이상한 게임이 섞임(계속 기준 조정 예정)
- 게임 리스트 작성 코드 최적화 
- 메타크리틱 빈 페이지 전송 대처
- 스팀API 쿼리 제한에 따른 게임 분배
- 크롤링 시간 범위 지정
- 고용량 파일을 패킷으로 저장


## 진행 상황

__크롤링 코드 작성__
- 상위 1000 게임 리스트 (완성)  
  API가 오후 12시부터 작동하는 제한 있음  
  순위에 있는 이상한 게임을 최대한 걸러내기 위해 기준 작성됨-조정 필요(슬랙 참조)  
  다양한 검증 기준으로 인해 약간 시간이 걸림-블랙리스트 작성 및 저장으로 속도 개선    
    
- 스팀(완성)  
  API 사용, 상당한 메타정보 존재(각 사이트에서 가져오는 정보는 코드 주석으로 정리되어 있음)  
  하루 제한: IP당 1000만 건, 시간당: 120만 건   
  예상 데이터 수: 적게는 5000, 많게는 백몇십만, 평균적으로 만건대 예상  
  데이터 종류: 게임 평가, 드립(재미있음 수가 높을 것), 트롤링  
    
  현재 발생한 에러: 네트워크 에러, api 에러. 최대 25초까지 대기 후 재시도하여 해결 중  
  로그 대신 print로 에러 출력 중  
  초기 대량 데이터 추출 완료. 공유 드라이브에 배포됨
 
 - 메타크리틱-유저(완성)  
   셀레니움 기반 크롤링 사용(메타크리틱은 동적 페이지를 활용함)
   속도가 꽤 느림. 100건당 3분쯤 소요됨
   게임당 많게는 n천 건, 적게는 수십 건 예상  
   데이터 종류: 모두 게임 평가(다만 의견은 극단적으로 갈릴 수 있음)  
   
   메타크리틱은 일정 주기동안 동일 ip에서 접속량이 초과될 시 빈 페이지를 띄움  
   빈 페이지를 감지 시 최소 10초~81초의 간격을 두고 재시도하여 해결 중  
   
 
__돌아가는 파이프라인 구축__
- aws 인스턴스 환경설정 및 사전 준비   
  task 분배를 위한 코드 작성 (스팀)
  새 인스턴스를 위한 requirements.txt 작성됨

## TODO
- 가능하다면(특히 디스코드), 다른 유저 포럼이나 리뷰 사이트 추가 크롤링 시도
  - 메타크리틱 전문가 콘솔 평가도 긁어오기
- 가져온 데이터들을 컬럼에 기반해 1차적으로 걸러낼 기준을 작성
- 좋아요 수가 일정 기간마다 쌓이는 문제 해결
- 가져온 지표들로 발표용 그래프 생성(할 만한 거리라도 메모해놓기)
